from snakemake.utils import Paramspace
import yaml

import pandas as pd

configfile: "config/config.yaml"

module vpipe_sudan:
    snakefile: github("cbg-ethz/V-pipe", path="workflow/Snakefile", tag="v3.0.0.pre1")
    config: config["vpipe_sudan"]
    replace_prefix: {"results/": "results_sudan/"}

use rule * from vpipe_sudan as vpipe_sudan_*

rule all:
    input:
        rules.vpipe_sudan_all.input,
        "results_sudan/csv_files/coverage.sudan.csv",
        "results_sudan/csv_files/mutations.sudan.csv",
        "results_sudan/csv_files/preprocessing.sudan.csv",
        "results_sudan/csv_files/align.sudan.csv",
    default_target: True

rule coverage_csv:
    input:
        fnames_coverage=expand("{dataset}/alignments/coverage.tsv.gz", dataset= vpipe_sudan.datasets),
    output:
        fname_coverage="results_sudan/csv_files/coverage.sudan.csv",
    run:
        import pandas as pd
        tmp = []
        for fname in input.fnames_coverage:
            df = pd.read_csv(fname, sep = '\t')
            sample_id = fname.split("/")[-4]+"/"+fname.split("/")[-3]
            df['coverage'] = df[sample_id]
            df = df[['ref', 'pos', 'coverage']]
            df['file'] = fname
            tmp.append(df)
        pd.concat(tmp).to_csv(output.fname_coverage)

rule preprocessing_csv:
    input:
        fnames=expand("{dataset}/preprocessed_data/prinseq.err.log", dataset= vpipe_sudan.datasets),
    output:
        fname="results_sudan/csv_files/preprocessing.sudan.csv",
    script:
        "scripts/preprocessing_csv.py"

rule align_csv:
    input:
        fnames=expand("{dataset}/alignments/REF_aln_stats.yaml", dataset= vpipe_sudan.datasets),
        bams=expand("{dataset}/alignments/REF_aln.bam", dataset= vpipe_sudan.datasets),
    output:
        fname="results_sudan/csv_files/align.sudan.csv",
    conda:
        "envs/viloca.yaml"
    script:
        "scripts/align_csv.py"

import os

def get_abs_bam(wildcards):
    return os.path.abspath(f"{wildcards.dataset}/alignments/REF_aln.bam")

rule viloca:
    input:
        fname_bam=get_abs_bam,
        fname_reference=lambda wildcards: os.path.abspath(config['vpipe_sudan']['input']['reference']),
    output:
        fname_csv="{dataset}/variants/cooccurring_mutations.csv",
        fname_vcf="{dataset}/variants/SNVs_0.010000_final.csv",
        dname_haplo=directory("{dataset}/variants/haplotypes"),
        dname_work=directory("{dataset}/variants/"),
    conda:
        "envs/viloca.yaml"
    params:
        window_size=100,
    resources:
        mem_mb=16*8000,
        runtime=14400,
    threads: 16
    shell:
        """
         # Ensure the working directory exists
        mkdir -p {output.dname_work} && \

        # Change to the working directory
        cd {output.dname_work} && \

        viloca run \
            -b {input.fname_bam} \
            -f {input.fname_reference} \
            --mode use_quality_scores \
            --alpha 0.0001 \
            --n_max_haplotypes 100 \
            -w {params.window_size} \
            --min_windows_coverage 1 \
            --win_min_ext 0.8 \
            --threads {threads} \
            --seed 2345 \
            --reuse_files
        """

rule mutations_csv:
    input:
        fnames_csv=expand("{dataset}/variants/SNVs_0.010000_final.csv", dataset= vpipe_sudan.datasets),
    output:
        fname_csv="results_sudan/csv_files/mutations.sudan.csv",
    run:
        import pandas as pd
        tmp = []
        for fname in input.fnames_csv:
            df = pd.read_csv(fname)
            df['file'] = fname
            tmp.append(df)
        pd.concat(tmp).to_csv(output.fname_csv)
